{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GRXF4ozrPmW",
        "outputId": "1c5e4715-20a5-4264-e205-5910e0dee1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.42.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray<0.8,>=0.6.11 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.7.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.42 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.42.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning>=0.42->pennylane) (0.3.30.0.2)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bM1-zDlNtCK5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pennylane as qml\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    Quantum filter pour extraire des features d'un patch 2x2\n",
        "\n",
        "    Param√®tres optimisables:\n",
        "    - params : Tensor de forme (n_layers, 2, n_qubits)\n",
        "      * params[layer, 0, qubit] : angle Œ∏ pour RY(Œ∏)\n",
        "      * params[layer, 1, qubit] : angle œÜ pour RZ(œÜ)\n",
        "\n",
        "    Pour n_layers=2, n_qubits=4 : 2 √ó 2 √ó 4 = 16 param√®tres par filtre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ‚ö†Ô∏è PARAM√àTRES QUANTIQUES OPTIMISABLES ‚ö†Ô∏è\n",
        "        Ces param√®tres seront mis √† jour par backpropagation !\n",
        "        Forme: (n_layers, 2, n_qubits)\n",
        "        - Dimension 0: layer du circuit (0 √† n_layers-1)\n",
        "        - Dimension 1: type de rotation (0=RY, 1=RZ)\n",
        "        - Dimension 2: qubit index (0 √† n_qubits-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Circuit quantique variationnel\n",
        "\n",
        "        Structure du circuit:\n",
        "        1. ENCODING: RX(Œ∏·µ¢) pour encoder les pixels\n",
        "        2. LAYER 1: CNOT ‚Üí RY(œÜ‚ÇÅ) ‚Üí RZ(œà‚ÇÅ)  [param√®tres optimisables]\n",
        "        3. LAYER 2: CNOT ‚Üí RY(œÜ‚ÇÇ) ‚Üí RZ(œà‚ÇÇ)  [param√®tres optimisables]\n",
        "        4. MEASUREMENT: ‚ü®Z·µ¢‚ü© pour chaque qubit\n",
        "\n",
        "        Param√®tres:\n",
        "        - inputs: angles d'encoding [Œ∏‚ÇÄ, Œ∏‚ÇÅ, Œ∏‚ÇÇ, Œ∏‚ÇÉ] (NON optimisables, viennent des pixels)\n",
        "        - params: angles variationnels (OPTIMISABLES via gradient descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Traite un patch 2x2 et retourne une feature scalaire\n",
        "\n",
        "        Args:\n",
        "            patch: Tensor de taille (4,) avec valeurs [0, 255]\n",
        "\n",
        "        Returns:\n",
        "            Feature scalaire agr√©g√©e (Tensor avec gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGFT-zMtwGY6"
      },
      "outputs": [],
      "source": [
        "class QuantumFilter(nn.Module):\n",
        "    def __init__(self, n_qubits: int = 4, n_layers: int = 2):\n",
        "        super(QuantumFilter, self).__init__()\n",
        "\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Cr√©ation du device quantique (simulateur)\n",
        "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
        "        self.params = nn.Parameter(\n",
        "            torch.tensor(\n",
        "                np.random.uniform(0, 2*np.pi, (n_layers, 2, n_qubits)),\n",
        "                dtype=torch.float32,\n",
        "                requires_grad=True  # ‚Üê Gradient activ√© !\n",
        "            )\n",
        "        )\n",
        "\n",
        "        print(f\"    QuantumFilter initialized: {self.params.numel()} trainable quantum parameters\")\n",
        "\n",
        "        self.qnode = qml.QNode(self._circuit, self.dev, interface='torch', diff_method='backprop')\n",
        "\n",
        "    def _circuit(self, inputs, params):\n",
        "        for i in range(self.n_qubits):\n",
        "            qml.RX(inputs[i], wires=i)\n",
        "\n",
        "        for layer in range(self.n_layers):\n",
        "            for i in range(self.n_qubits - 1):\n",
        "                qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.RY(params[layer, 0, i], wires=i)\n",
        "\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.RZ(params[layer, 1, i], wires=i)\n",
        "\n",
        "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
        "\n",
        "    def forward(self, patch: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        theta = (patch / 255.0) * 2 * np.pi\n",
        "\n",
        "        measurements = self.qnode(theta, self.params)\n",
        "\n",
        "        feature = sum(measurements) / len(measurements)\n",
        "\n",
        "        return feature  \n",
        "\n",
        "    def get_params(self) -> torch.Tensor:\n",
        "        \"\"\"Retourne les param√®tres quantiques\"\"\"\n",
        "        return self.params.data\n",
        "\n",
        "    def set_params(self, params: torch.Tensor):\n",
        "        \"\"\"Met √† jour les param√®tres quantiques\"\"\"\n",
        "        self.params.data = params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    Couche quanvolutionnelle compl√®te avec N filtres quantiques\n",
        "\n",
        "    Chaque filtre a ses propres param√®tres VQC optimisables\n",
        "    Pour n_filters=8, n_layers=2, n_qubits=4:\n",
        "    Total param√®tres quantiques = 8 √ó (2 √ó 2 √ó 4) = 128 param√®tres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Applique tous les filtres quantiques √† l'image\n",
        "\n",
        "        Args:\n",
        "            image: Tensor de taille (32, 32, 1) - grayscale\n",
        "\n",
        "        Returns:\n",
        "            Feature map de taille (16, 16, n_filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y24JRu6FwBbV"
      },
      "outputs": [],
      "source": [
        "class QuanvolutionalLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, n_filters: int = 8, n_qubits: int = 4, n_layers: int = 2):\n",
        "        super(QuanvolutionalLayer, self).__init__()\n",
        "\n",
        "        self.n_filters = n_filters\n",
        "\n",
        "        self.filters = nn.ModuleList([\n",
        "            QuantumFilter(n_qubits, n_layers) for _ in range(n_filters)\n",
        "        ])\n",
        "\n",
        "        total_quantum_params = sum(f.params.numel() for f in self.filters)\n",
        "        print(f\"  QuanvolutionalLayer: {n_filters} filters √ó {n_qubits} qubits √ó {n_layers} layers\")\n",
        "        print(f\"  Total quantum parameters: {total_quantum_params}\")\n",
        "\n",
        "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        H, W = image.shape[:2]\n",
        "        patch_size = 2\n",
        "        stride = 2\n",
        "\n",
        "        out_h = (H - patch_size) // stride + 1  \n",
        "        out_w = (W - patch_size) // stride + 1  \n",
        "\n",
        "        features_list = []\n",
        "\n",
        "        for filter_idx in range(self.n_filters):\n",
        "            print(f\"  Processing filter {filter_idx + 1}/{self.n_filters}...\", end='\\r')\n",
        "\n",
        "            filter_output = torch.zeros(out_h, out_w)\n",
        "\n",
        "            for i in range(out_h):\n",
        "                for j in range(out_w):\n",
        "                    # Extraction du patch 2x2\n",
        "                    h_start = i * stride\n",
        "                    w_start = j * stride\n",
        "                    patch = image[h_start:h_start+patch_size, w_start:w_start+patch_size, 0]\n",
        "\n",
        "                    patch_flat = patch.flatten()\n",
        "\n",
        "                    feature = self.filters[filter_idx].forward(patch_flat)\n",
        "                    filter_output[i, j] = feature\n",
        "\n",
        "            features_list.append(filter_output)\n",
        "\n",
        "        print()  \n",
        "\n",
        "        # Stack pour obtenir (n_filters, 16, 16) puis permute vers (16, 16, n_filters)\n",
        "        feature_map = torch.stack(features_list, dim=0)  # Shape: (n_filters, 16, 16)\n",
        "        feature_map = feature_map.permute(1, 2, 0)  # Shape: (16, 16, n_filters)\n",
        "\n",
        "        return feature_map.float()\n",
        "\n",
        "    def get_all_params(self) -> List[torch.Tensor]:\n",
        "        return [f.get_params() for f in self.filters]\n",
        "\n",
        "    def set_all_params(self, params_list: List[torch.Tensor]):\n",
        "        for i, params in enumerate(params_list):\n",
        "            self.filters[i].set_params(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    Quanvolutional Neural Network compl√®te\n",
        "\n",
        "    PARAM√àTRES OPTIMISABLES:\n",
        "    1. Param√®tres quantiques (VQC):\n",
        "       - n_filters √ó n_layers √ó 2 √ó n_qubits param√®tres\n",
        "       - Exemple: 8 √ó 2 √ó 2 √ó 4 = 128 param√®tres quantiques\n",
        "\n",
        "    2. Param√®tres classiques (FC layers):\n",
        "       - FC1: (256√ón_filters) √ó 128 + 128 bias\n",
        "       - FC2: 128 √ó 64 + 64 bias\n",
        "       - FC3: 64 √ó n_classes + n_classes bias\n",
        "       - Exemple pour n_filters=8, n_classes=10:\n",
        "         (2048√ó128 + 128) + (128√ó64 + 64) + (64√ó10 + 10) = 270,666 param√®tres\n",
        "\n",
        "    TOTAL: ~128 quantiques + ~270k classiques = ~270k param√®tres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Convertit RGB en grayscale\n",
        "\n",
        "        Args:\n",
        "            image: Tensor de taille (32, 32, 3)\n",
        "\n",
        "        Returns:\n",
        "            Tensor de taille (32, 32, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Forward pass complet\n",
        "\n",
        "        Args:\n",
        "            x: Batch d'images de taille (batch_size, 32, 32, 3)\n",
        "\n",
        "        Returns:\n",
        "            Probabilit√©s de classe (batch_size, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duoimBC6ti5m"
      },
      "outputs": [],
      "source": [
        "class QuanNN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_filters: int = 8, n_classes: int = 10,\n",
        "                 n_qubits: int = 4, n_layers: int = 2):\n",
        "        super(QuanNN, self).__init__()\n",
        "\n",
        "        self.n_filters = n_filters\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Initializing QuanNN Model\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Couche quanvolutionnelle\n",
        "        self.quanv_layer = QuanvolutionalLayer(n_filters, n_qubits, n_layers)\n",
        "\n",
        "        flatten_dim = 256 * n_filters\n",
        "\n",
        "        # Couches fully connected classiques\n",
        "        self.fc1 = nn.Linear(flatten_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, n_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Calcul du nombre de param√®tres\n",
        "        quantum_params = sum(p.numel() for p in self.quanv_layer.parameters())\n",
        "        classical_params = sum(p.numel() for p in [*self.fc1.parameters(),\n",
        "                                                     *self.fc2.parameters(),\n",
        "                                                     *self.fc3.parameters()])\n",
        "        total_params = quantum_params + classical_params\n",
        "\n",
        "        print(f\"\\n  Parameter Breakdown:\")\n",
        "        print(f\"  {'‚îÄ'*58}\")\n",
        "        print(f\"  Quantum (VQC) parameters:    {quantum_params:>10,}\")\n",
        "        print(f\"  Classical (FC) parameters:   {classical_params:>10,}\")\n",
        "        print(f\"  {'‚îÄ'*58}\")\n",
        "        print(f\"  TOTAL trainable parameters:  {total_params:>10,}\")\n",
        "        print(f\"  {'='*60}\\n\")\n",
        "\n",
        "    def rgb_to_grayscale(self, image: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        if image.shape[2] == 3:\n",
        "            # Formule de conversion: 0.299*R + 0.587*G + 0.114*B\n",
        "            weights = torch.tensor([0.299, 0.587, 0.114], dtype=image.dtype)\n",
        "            gray = torch.sum(image * weights, dim=2, keepdim=True)\n",
        "            return gray\n",
        "        return image\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = x.shape[0]\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            print(f\"Processing image {i+1}/{batch_size}\")\n",
        "\n",
        "            # Step 1: Conversion en grayscale\n",
        "            img = x[i]\n",
        "            gray_img = self.rgb_to_grayscale(img)\n",
        "\n",
        "            # Step 2-5: Application de la couche quanvolutionnelle\n",
        "            feature_map = self.quanv_layer.forward(gray_img)\n",
        "            print(f\"  Shape after quanv_layer: {feature_map.shape}\")\n",
        "\n",
        "            # Step 6: Flatten - Reshape en vecteur 1D\n",
        "            # feature_map shape: (16, 16, n_filters)\n",
        "            # After flatten: (16 * 16 * n_filters) = (256 * n_filters)\n",
        "            features = feature_map.reshape(-1)  # Flatten all dimensions\n",
        "            print(f\"  Shape after flatten: {features.shape}\")\n",
        "\n",
        "            outputs.append(features)\n",
        "\n",
        "        # Stack all outputs\n",
        "        batch_features = torch.stack(outputs).float()\n",
        "        print(f\"Shape after stacking batch features: {batch_features.shape}\")\n",
        "\n",
        "        # Fully connected layers\n",
        "        h1 = self.relu(self.fc1(batch_features))\n",
        "        h2 = self.relu(self.fc2(h1))\n",
        "        logits = self.fc3(h2)\n",
        "\n",
        "        # Softmax for probabilities\n",
        "        probs = self.softmax(logits)\n",
        "\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rko7UtMPtfn9"
      },
      "outputs": [],
      "source": [
        "def test_quannn(model: QuanNN, test_loader: DataLoader, device='cpu'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    test_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n=== Testing QuanNN ===\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            print(f\"Testing batch {batch_idx + 1}/{len(test_loader)}...\", end='\\r')\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calcul de la loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Pr√©dictions\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Statistiques\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Sauvegarde pour matrice de confusion\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calcul des m√©triques\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "\n",
        "    print(f\"\\n\\n{'='*50}\")\n",
        "    print(f\"Test Results:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    return accuracy, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu3WtEVWtcdB"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, class_names=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibKJB4qftWe0"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(train_losses, train_accs, val_losses=None, val_accs=None):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss\n",
        "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
        "    if val_losses:\n",
        "        ax1.plot(val_losses, label='Val Loss', marker='s')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Accuracy\n",
        "    ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
        "    if val_accs:\n",
        "        ax2.plot(val_accs, label='Val Accuracy', marker='s')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    print(\"Training history saved as 'training_history.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvG9P3I5tUyK"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model: QuanNN, test_loader: DataLoader,\n",
        "                         class_names=None, n_images=10):\n",
        "    model.eval()\n",
        "    images, labels = next(iter(test_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images[:n_images])\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "    # Affichage\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(min(n_images, len(images))):\n",
        "        img = images[i].cpu().numpy()\n",
        "\n",
        "        # Conversion pour affichage\n",
        "        if img.shape[2] == 3:\n",
        "            img = img.astype(np.uint8)\n",
        "        else:\n",
        "            img = img[:, :, 0].astype(np.uint8)\n",
        "\n",
        "        axes[i].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
        "\n",
        "        true_label = class_names[labels[i]] if class_names else labels[i].item()\n",
        "        pred_label = class_names[predictions[i]] if class_names else predictions[i].item()\n",
        "\n",
        "        color = 'green' if predictions[i] == labels[i] else 'red'\n",
        "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions_visualization.png')\n",
        "    print(\"Predictions visualization saved as 'predictions_visualization.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45fvQRY4tThM"
      },
      "outputs": [],
      "source": [
        "def load_cifar10_subset(n_classes=3, n_train=100, n_test=30):\n",
        "    print(f\"\\n=== Loading CIFAR-10 Subset ===\")\n",
        "    print(f\"Classes: {n_classes}, Train samples: {n_train}, Test samples: {n_test}\")\n",
        "\n",
        "    # Transformation: redimensionne √† 32x32 (d√©j√† la bonne taille pour CIFAR-10)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x * 255)  # Pixel values [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Chargement de CIFAR-10\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                           download=True, transform=transform)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                          download=True, transform=transform)\n",
        "\n",
        "    # S√©lection des n_classes premi√®res classes\n",
        "    class_indices_train = [i for i, label in enumerate(trainset.targets) if label < n_classes]\n",
        "    class_indices_test = [i for i, label in enumerate(testset.targets) if label < n_classes]\n",
        "\n",
        "    # Sous-√©chantillonnage\n",
        "    selected_train = class_indices_train[:n_train]\n",
        "    selected_test = class_indices_test[:n_test]\n",
        "\n",
        "    # Cr√©ation des sous-ensembles\n",
        "    train_subset = torch.utils.data.Subset(trainset, selected_train)\n",
        "    test_subset = torch.utils.data.Subset(testset, selected_test)\n",
        "\n",
        "    # Noms des classes\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck'][:n_classes]\n",
        "\n",
        "    print(f\"Loaded: {len(train_subset)} train, {len(test_subset)} test samples\")\n",
        "    print(f\"Classes: {class_names}\\n\")\n",
        "\n",
        "    return train_subset, test_subset, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CtWbou9tSKX"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model: QuanNN, test_loader: DataLoader,\n",
        "                         class_names=None, n_images=10):\n",
        "    model.eval()\n",
        "    images, labels = next(iter(test_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images[:n_images])\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "    # Affichage\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(min(n_images, len(images))):\n",
        "        img = images[i].cpu().numpy()\n",
        "\n",
        "        # Conversion pour affichage\n",
        "        if img.shape[2] == 3:\n",
        "            img = img.astype(np.uint8)\n",
        "        else:\n",
        "            img = img[:, :, 0].astype(np.uint8)\n",
        "\n",
        "        axes[i].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
        "\n",
        "        true_label = class_names[labels[i]] if class_names else labels[i].item()\n",
        "        pred_label = class_names[predictions[i]] if class_names else predictions[i].item()\n",
        "\n",
        "        color = 'green' if predictions[i] == labels[i] else 'red'\n",
        "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions_visualization.png')\n",
        "    print(\"Predictions visualization saved as 'predictions_visualization.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbeKmaJ8tQkZ"
      },
      "outputs": [],
      "source": [
        "def load_mnist_subset(n_classes=3, n_train=100, n_test=30):\n",
        "    print(f\"\\n=== Loading MNIST Subset ===\")\n",
        "    print(f\"Classes: {n_classes}, Train samples: {n_train}, Test samples: {n_test}\")\n",
        "\n",
        "    # Transformation: redimensionne √† 32x32 et convertit en RGB avec le bon format (H, W, C)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),  # Donne (1, 32, 32)\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # (3, 32, 32)\n",
        "        transforms.Lambda(lambda x: x.permute(1, 2, 0)),  # (32, 32, 3) - AJOUT CRUCIAL\n",
        "        transforms.Lambda(lambda x: x * 255)  # Pixel values [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Chargement de MNIST\n",
        "    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                         download=True, transform=transform)\n",
        "    testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "    # S√©lection des n_classes premi√®res classes\n",
        "    class_indices_train = [i for i, label in enumerate(trainset.targets) if label < n_classes]\n",
        "    class_indices_test = [i for i, label in enumerate(testset.targets) if label < n_classes]\n",
        "\n",
        "    # Sous-√©chantillonnage\n",
        "    selected_train = class_indices_train[:n_train]\n",
        "    selected_test = class_indices_test[:n_test]\n",
        "\n",
        "    # Cr√©ation des sous-ensembles\n",
        "    train_subset = torch.utils.data.Subset(trainset, selected_train)\n",
        "    test_subset = torch.utils.data.Subset(testset, selected_test)\n",
        "\n",
        "    class_names = [str(i) for i in range(n_classes)]\n",
        "\n",
        "    print(f\"Loaded: {len(train_subset)} train, {len(test_subset)} test samples\")\n",
        "    print(f\"Classes: {class_names}\\n\")\n",
        "\n",
        "    return train_subset, test_subset, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ZHeTuItO5T"
      },
      "outputs": [],
      "source": [
        "def save_model(model: QuanNN, filepath='quannn_model.pth'):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),  # Contient TOUS les param√®tres\n",
        "        'quantum_params': model.quanv_layer.get_all_params(),\n",
        "        'n_filters': model.n_filters,\n",
        "        'n_classes': model.n_classes\n",
        "    }, filepath)\n",
        "    print(f\"‚úì Model saved to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qB9QmrAtNJ9"
      },
      "outputs": [],
      "source": [
        "def load_model(filepath='quannn_model.pth', n_qubits=4, n_layers=2):\n",
        "    checkpoint = torch.load(filepath)\n",
        "\n",
        "    model = QuanNN(\n",
        "        n_filters=checkpoint['n_filters'],\n",
        "        n_classes=checkpoint['n_classes'],\n",
        "        n_qubits=n_qubits,\n",
        "        n_layers=n_layers\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.quanv_layer.set_all_params(checkpoint['quantum_params'])\n",
        "\n",
        "    print(f\"‚úì Model loaded from {filepath}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1qbhuwftLq4"
      },
      "outputs": [],
      "source": [
        "def print_parameter_details(model: QuanNN):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DETAILED PARAMETER ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nüìä QUANTUM PARAMETERS (VQC):\")\n",
        "    print(\"‚îÄ\"*70)\n",
        "    for idx, qfilter in enumerate(model.quanv_layer.filters):\n",
        "        params = qfilter.params\n",
        "        print(f\"  Filter {idx+1}:\")\n",
        "        print(f\"    Shape: {params.shape} = (layers={params.shape[0]}, \"\n",
        "              f\"rotations={params.shape[1]}, qubits={params.shape[2]})\")\n",
        "        print(f\"    Total: {params.numel()} parameters\")\n",
        "        print(f\"    Requires grad: {params.requires_grad}\")\n",
        "        print(f\"    Example values (first layer, RY rotations):\")\n",
        "        print(f\"      {params[0, 0, :].detach().numpy()}\")\n",
        "        if idx == 2:  # Limite l'affichage\n",
        "            print(f\"    ... ({model.n_filters - 3} more filters)\")\n",
        "            break\n",
        "\n",
        "    quantum_total = sum(f.params.numel() for f in model.quanv_layer.filters)\n",
        "    print(f\"\\n  ‚úì Total Quantum Params: {quantum_total}\")\n",
        "\n",
        "    print(\"\\nüìä CLASSICAL PARAMETERS (Fully Connected):\")\n",
        "    print(\"‚îÄ\"*70)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'fc' in name:\n",
        "            print(f\"  {name}:\")\n",
        "            print(f\"    Shape: {param.shape}\")\n",
        "            print(f\"    Total: {param.numel()} parameters\")\n",
        "            print(f\"    Requires grad: {param.requires_grad}\")\n",
        "\n",
        "    classical_total = sum(p.numel() for name, p in model.named_parameters() if 'fc' in name)\n",
        "    print(f\"\\n  ‚úì Total Classical Params: {classical_total}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"GRAND TOTAL: {quantum_total + classical_total:,} trainable parameters\")\n",
        "    print(f\"  - Quantum: {quantum_total:,} ({100*quantum_total/(quantum_total+classical_total):.2f}%)\")\n",
        "    print(f\"  - Classical: {classical_total:,} ({100*classical_total/(quantum_total+classical_total):.2f}%)\")\n",
        "    print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJls2P4VtKbe"
      },
      "outputs": [],
      "source": [
        "def run_complete_experiment(dataset='cifar10', n_filters=4, n_classes=3,\n",
        "                           n_train=60, n_test=20, n_epochs=3, batch_size=2):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"QuanNN - Complete Experiment\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Chargement des donn√©es\n",
        "    if dataset == 'cifar10':\n",
        "        train_data, test_data, class_names = load_cifar10_subset(\n",
        "            n_classes=n_classes, n_train=n_train, n_test=n_test\n",
        "        )\n",
        "    elif dataset == 'mnist':\n",
        "        train_data, test_data, class_names = load_mnist_subset(\n",
        "            n_classes=n_classes, n_train=n_train, n_test=n_test\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Dataset must be 'cifar10' or 'mnist'\")\n",
        "\n",
        "    # Cr√©ation des dataloaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # 2. Cr√©ation du mod√®le\n",
        "    print(f\"\\nCreating QuanNN model...\")\n",
        "    print(f\"  - Quantum filters: {n_filters}\")\n",
        "    print(f\"  - Classes: {n_classes}\")\n",
        "    print(f\"  - VQC layers: 2\")\n",
        "\n",
        "    model = QuanNN(n_filters=n_filters, n_classes=n_classes,\n",
        "                   n_qubits=4, n_layers=2)\n",
        "\n",
        "    # Affichage de l'architecture\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  - Total trainable parameters: {total_params:,}\")\n",
        "\n",
        "    # 3. Entra√Ænement\n",
        "    train_losses, train_accs, val_losses, val_accs = train_quannn(\n",
        "        model, train_loader, val_loader=test_loader,\n",
        "        n_epochs=n_epochs, lr=0.01\n",
        "    )\n",
        "\n",
        "    # 4. Test final\n",
        "    accuracy, predictions, labels = test_quannn(model, test_loader)\n",
        "\n",
        "    # 5. Visualisations\n",
        "    print(\"\\n=== Generating Visualizations ===\")\n",
        "    plot_confusion_matrix(labels, predictions, class_names)\n",
        "    visualize_predictions(model, test_loader, class_names, n_images=10)\n",
        "\n",
        "    # 6. Rapport de classification\n",
        "    print(\"\\n=== Classification Report ===\")\n",
        "    print(classification_report(labels, predictions,\n",
        "                                target_names=class_names,\n",
        "                                zero_division=0))\n",
        "\n",
        "    # 7. Sauvegarde du mod√®le\n",
        "    save_model(model, f'quannn_{dataset}_model.pth')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Experiment Complete!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nFiles generated:\")\n",
        "    print(f\"  - confusion_matrix.png\")\n",
        "    print(f\"  - training_history.png\")\n",
        "    print(f\"  - predictions_visualization.png\")\n",
        "    print(f\"  - quannn_{dataset}_model.pth\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return model, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FGXia_putIPV"
      },
      "outputs": [],
      "source": [
        "def train_quannn(model: QuanNN, train_loader: DataLoader,\n",
        "                 val_loader: DataLoader = None, n_epochs: int = 5,\n",
        "                 lr: float = 0.01, device='cuda'):\n",
        "    \"\"\"\n",
        "    Entra√Æne le mod√®le QuanNN avec suivi complet\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    print(\"\\n=== Training QuanNN ===\\n\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # === TRAINING ===\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{n_epochs}] - Training...\")\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calcul de la loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistiques\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f}\", end='\\r')\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_accuracy)\n",
        "\n",
        "        print(f\"\\n  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # === VALIDATION ===\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            print(f\"  Validating...\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_accuracy = 100 * val_correct / val_total\n",
        "            val_loss_avg = val_loss / len(val_loader)\n",
        "            val_losses.append(val_loss_avg)\n",
        "            val_accs.append(val_accuracy)\n",
        "\n",
        "            print(f\"  Val Loss: {val_loss_avg:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        print(f\"{'-'*60}\\n\")\n",
        "\n",
        "    print(\"=== Training Complete ===\\n\")\n",
        "\n",
        "    # Affichage de l'historique\n",
        "    if val_loader:\n",
        "        plot_training_history(train_losses, train_accs, val_losses, val_accs)\n",
        "    else:\n",
        "        plot_training_history(train_losses, train_accs)\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywyQbvWtFPX",
        "outputId": "dd421937-d550-45ac-906c-ea60a1a752c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QuanNN - Testing Options\n",
            "============================================================\n",
            "\n",
            "Choose an option:\n",
            "  1. Quick test (CIFAR-10, 3 classes, 60 train/20 test)\n",
            "  2. MNIST test (3 classes, 60 train/20 test)\n",
            "  3. Extended CIFAR-10 (5 classes, 200 train/50 test)\n",
            "  4. Load and test existing model\n",
            "  5. Simple forward pass demo\n",
            "\n",
            "Enter choice (1-5): 2\n",
            "\n",
            "[Option 2] MNIST Test\n",
            "\n",
            "============================================================\n",
            "QuanNN - Complete Experiment\n",
            "============================================================\n",
            "\n",
            "=== Loading MNIST Subset ===\n",
            "Classes: 3, Train samples: 20, Test samples: 10\n",
            "Loaded: 20 train, 10 test samples\n",
            "Classes: ['0', '1', '2']\n",
            "\n",
            "\n",
            "Creating QuanNN model...\n",
            "  - Quantum filters: 2\n",
            "  - Classes: 3\n",
            "  - VQC layers: 2\n",
            "\n",
            "============================================================\n",
            "Initializing QuanNN Model\n",
            "============================================================\n",
            "    QuantumFilter initialized: 16 trainable quantum parameters\n",
            "    QuantumFilter initialized: 16 trainable quantum parameters\n",
            "  QuanvolutionalLayer: 2 filters √ó 4 qubits √ó 2 layers\n",
            "  Total quantum parameters: 32\n",
            "\n",
            "  Parameter Breakdown:\n",
            "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  Quantum (VQC) parameters:            32\n",
            "  Classical (FC) parameters:       74,115\n",
            "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  TOTAL trainable parameters:      74,147\n",
            "  ============================================================\n",
            "\n",
            "  - Total trainable parameters: 74,147\n",
            "\n",
            "=== Training QuanNN ===\n",
            "\n",
            "Epoch [1/2] - Training...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Batch 10/10 - Loss: 1.2970\n",
            "  Train Loss: 1.0962, Train Accuracy: 45.00%\n",
            "  Validating...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Val Loss: 1.0385, Val Accuracy: 40.00%\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch [2/2] - Training...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Batch 10/10 - Loss: 0.7122\n",
            "  Train Loss: 0.9723, Train Accuracy: 60.00%\n",
            "  Validating...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Val Loss: 0.9741, Val Accuracy: 60.00%\n",
            "------------------------------------------------------------\n",
            "\n",
            "=== Training Complete ===\n",
            "\n",
            "Training history saved as 'training_history.png'\n",
            "\n",
            "=== Testing QuanNN ===\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "\n",
            "\n",
            "==================================================\n",
            "Test Results:\n",
            "==================================================\n",
            "Test Loss: 0.9741\n",
            "Test Accuracy: 60.00% (6/10)\n",
            "==================================================\n",
            "\n",
            "\n",
            "=== Generating Visualizations ===\n",
            "Confusion matrix saved as 'confusion_matrix.png'\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Predictions visualization saved as 'predictions_visualization.png'\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.40      0.57         5\n",
            "           1       0.50      1.00      0.67         4\n",
            "           2       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.60        10\n",
            "   macro avg       0.50      0.47      0.41        10\n",
            "weighted avg       0.70      0.60      0.55        10\n",
            "\n",
            "‚úì Model saved to quannn_mnist_model.pth\n",
            "\n",
            "============================================================\n",
            "Experiment Complete!\n",
            "============================================================\n",
            "\n",
            "Files generated:\n",
            "  - confusion_matrix.png\n",
            "  - training_history.png\n",
            "  - predictions_visualization.png\n",
            "  - quannn_mnist_model.pth\n",
            "\n",
            "\n",
            "\n",
            "============================================================\n",
            "Done!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"QuanNN - Testing Options\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nChoose an option:\")\n",
        "    print(\"  1. Quick test (CIFAR-10, 3 classes, 60 train/20 test)\")\n",
        "    print(\"  2. MNIST test (3 classes, 60 train/20 test)\")\n",
        "    print(\"  3. Extended CIFAR-10 (5 classes, 200 train/50 test)\")\n",
        "    print(\"  4. Load and test existing model\")\n",
        "    print(\"  5. Simple forward pass demo\")\n",
        "\n",
        "    choice = input(\"\\nEnter choice (1-5): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        # Test rapide avec CIFAR-10\n",
        "        print(\"\\n[Option 1] Quick CIFAR-10 Test\")\n",
        "        model, accuracy = run_complete_experiment(\n",
        "            dataset='cifar10',\n",
        "            n_filters=4,\n",
        "            n_classes=3,\n",
        "            n_train=60,\n",
        "            n_test=20,\n",
        "            n_epochs=3,\n",
        "            batch_size=2\n",
        "        )\n",
        "\n",
        "    elif choice == '2':\n",
        "        # Test avec MNIST\n",
        "        print(\"\\n[Option 2] MNIST Test\")\n",
        "        model, accuracy = run_complete_experiment(\n",
        "            dataset='mnist',\n",
        "            n_filters=2,\n",
        "            n_classes=3,\n",
        "            n_train=20,\n",
        "            n_test=10,\n",
        "            n_epochs=2,\n",
        "            batch_size=2\n",
        "        )\n",
        "\n",
        "    elif choice == '3':\n",
        "        # Test √©tendu\n",
        "        print(\"\\n[Option 3] Extended CIFAR-10 Test\")\n",
        "        print(\"WARNING: This will take significantly longer!\")\n",
        "        confirm = input(\"Continue? (y/n): \").strip().lower()\n",
        "        if confirm == 'y':\n",
        "            model, accuracy = run_complete_experiment(\n",
        "                dataset='cifar10',\n",
        "                n_filters=8,\n",
        "                n_classes=5,\n",
        "                n_train=200,\n",
        "                n_test=50,\n",
        "                n_epochs=5,\n",
        "                batch_size=2\n",
        "            )\n",
        "\n",
        "    elif choice == '4':\n",
        "        # Chargement et test d'un mod√®le existant\n",
        "        print(\"\\n[Option 4] Load Existing Model\")\n",
        "        filepath = input(\"Enter model path (default: quannn_cifar10_model.pth): \").strip()\n",
        "        if not filepath:\n",
        "            filepath = 'quannn_cifar10_model.pth'\n",
        "\n",
        "        try:\n",
        "            model = load_model(filepath)\n",
        "\n",
        "            # Chargement des donn√©es de test\n",
        "            _, test_data, class_names = load_cifar10_subset(n_classes=3, n_test=20)\n",
        "            test_loader = DataLoader(test_data, batch_size=2, shuffle=False)\n",
        "\n",
        "            # Test\n",
        "            accuracy, predictions, labels = test_quannn(model, test_loader)\n",
        "            plot_confusion_matrix(labels, predictions, class_names)\n",
        "            visualize_predictions(model, test_loader, class_names)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Model file '{filepath}' not found!\")\n",
        "\n",
        "    elif choice == '5':\n",
        "        # D√©mo simple du forward pass\n",
        "        print(\"\\n[Option 5] Simple Forward Pass Demo\")\n",
        "\n",
        "        # Cr√©ation d'une image factice\n",
        "        print(\"\\nCreating random test image (32x32x3)...\")\n",
        "        test_img = torch.rand(1, 32, 32, 3) * 255\n",
        "\n",
        "        # Cr√©ation du mod√®le\n",
        "        print(\"Creating QuanNN model (4 filters, 3 classes)...\")\n",
        "        model = QuanNN(n_filters=4, n_classes=3, n_qubits=4, n_layers=2)\n",
        "        # Affichage d√©taill√© des param√®tres\n",
        "        print_parameter_details(model)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            output = model(test_img)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Results:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Output shape: {output.shape}\")\n",
        "        print(f\"Class probabilities: {output[0].numpy()}\")\n",
        "        print(f\"Predicted class: {torch.argmax(output[0]).item()}\")\n",
        "        print(f\"Confidence: {torch.max(output[0]).item():.2%}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Forward pass avec gradient\n",
        "        output = model(test_img)\n",
        "        loss = -torch.log(output[0, 0])  # Simple loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        print(\"\\n‚úì Gradient check:\")\n",
        "        has_quantum_grad = any(f.params.grad is not None for f in model.quanv_layer.filters)\n",
        "        has_classical_grad = model.fc1.weight.grad is not None\n",
        "\n",
        "        print(f\"  Quantum params have gradients: {has_quantum_grad}\")\n",
        "        print(f\"  Classical params have gradients: {has_classical_grad}\")\n",
        "\n",
        "        if has_quantum_grad:\n",
        "            print(f\"\\n  Example quantum gradient (Filter 1, Layer 1, RY):\")\n",
        "            print(f\"    {model.quanv_layer.filters[0].params.grad[0, 0, :].numpy()}\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"\\nInvalid choice! Running default demo...\")\n",
        "        print(\"\\n[Default] Simple Forward Pass Demo\")\n",
        "        test_img = torch.rand(1, 32, 32, 3) * 255\n",
        "        model = QuanNN(n_filters=4, n_classes=3, n_qubits=4, n_layers=2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(test_img)\n",
        "\n",
        "        print(f\"\\nOutput probabilities: {output[0].numpy()}\")\n",
        "        print(f\"Predicted class: {torch.argmax(output[0]).item()}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
