{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GRXF4ozrPmW",
        "outputId": "1c5e4715-20a5-4264-e205-5910e0dee1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.12/dist-packages (0.42.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.17.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray<0.8,>=0.6.11 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.7.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.42 in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.42.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from pennylane-lightning>=0.42->pennylane) (0.3.30.0.2)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bM1-zDlNtCK5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pennylane as qml\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    Quantum filter pour extraire des features d'un patch 2x2\n",
        "\n",
        "    Paramètres optimisables:\n",
        "    - params : Tensor de forme (n_layers, 2, n_qubits)\n",
        "      * params[layer, 0, qubit] : angle θ pour RY(θ)\n",
        "      * params[layer, 1, qubit] : angle φ pour RZ(φ)\n",
        "\n",
        "    Pour n_layers=2, n_qubits=4 : 2 × 2 × 4 = 16 paramètres par filtre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        ⚠️ PARAMÈTRES QUANTIQUES OPTIMISABLES ⚠️\n",
        "        Ces paramètres seront mis à jour par backpropagation !\n",
        "        Forme: (n_layers, 2, n_qubits)\n",
        "        - Dimension 0: layer du circuit (0 à n_layers-1)\n",
        "        - Dimension 1: type de rotation (0=RY, 1=RZ)\n",
        "        - Dimension 2: qubit index (0 à n_qubits-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Circuit quantique variationnel\n",
        "\n",
        "        Structure du circuit:\n",
        "        1. ENCODING: RX(θᵢ) pour encoder les pixels\n",
        "        2. LAYER 1: CNOT → RY(φ₁) → RZ(ψ₁)  [paramètres optimisables]\n",
        "        3. LAYER 2: CNOT → RY(φ₂) → RZ(ψ₂)  [paramètres optimisables]\n",
        "        4. MEASUREMENT: ⟨Zᵢ⟩ pour chaque qubit\n",
        "\n",
        "        Paramètres:\n",
        "        - inputs: angles d'encoding [θ₀, θ₁, θ₂, θ₃] (NON optimisables, viennent des pixels)\n",
        "        - params: angles variationnels (OPTIMISABLES via gradient descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Traite un patch 2x2 et retourne une feature scalaire\n",
        "\n",
        "        Args:\n",
        "            patch: Tensor de taille (4,) avec valeurs [0, 255]\n",
        "\n",
        "        Returns:\n",
        "            Feature scalaire agrégée (Tensor avec gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGFT-zMtwGY6"
      },
      "outputs": [],
      "source": [
        "class QuantumFilter(nn.Module):\n",
        "    def __init__(self, n_qubits: int = 4, n_layers: int = 2):\n",
        "        super(QuantumFilter, self).__init__()\n",
        "\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Création du device quantique (simulateur)\n",
        "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
        "        self.params = nn.Parameter(\n",
        "            torch.tensor(\n",
        "                np.random.uniform(0, 2*np.pi, (n_layers, 2, n_qubits)),\n",
        "                dtype=torch.float32,\n",
        "                requires_grad=True  # ← Gradient activé !\n",
        "            )\n",
        "        )\n",
        "\n",
        "        print(f\"    QuantumFilter initialized: {self.params.numel()} trainable quantum parameters\")\n",
        "\n",
        "        self.qnode = qml.QNode(self._circuit, self.dev, interface='torch', diff_method='backprop')\n",
        "\n",
        "    def _circuit(self, inputs, params):\n",
        "        for i in range(self.n_qubits):\n",
        "            qml.RX(inputs[i], wires=i)\n",
        "\n",
        "        for layer in range(self.n_layers):\n",
        "            for i in range(self.n_qubits - 1):\n",
        "                qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.RY(params[layer, 0, i], wires=i)\n",
        "\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.RZ(params[layer, 1, i], wires=i)\n",
        "\n",
        "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
        "\n",
        "    def forward(self, patch: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        theta = (patch / 255.0) * 2 * np.pi\n",
        "\n",
        "        measurements = self.qnode(theta, self.params)\n",
        "\n",
        "        feature = sum(measurements) / len(measurements)\n",
        "\n",
        "        return feature  \n",
        "\n",
        "    def get_params(self) -> torch.Tensor:\n",
        "        \"\"\"Retourne les paramètres quantiques\"\"\"\n",
        "        return self.params.data\n",
        "\n",
        "    def set_params(self, params: torch.Tensor):\n",
        "        \"\"\"Met à jour les paramètres quantiques\"\"\"\n",
        "        self.params.data = params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    Couche quanvolutionnelle complète avec N filtres quantiques\n",
        "\n",
        "    Chaque filtre a ses propres paramètres VQC optimisables\n",
        "    Pour n_filters=8, n_layers=2, n_qubits=4:\n",
        "    Total paramètres quantiques = 8 × (2 × 2 × 4) = 128 paramètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Applique tous les filtres quantiques à l'image\n",
        "\n",
        "        Args:\n",
        "            image: Tensor de taille (32, 32, 1) - grayscale\n",
        "\n",
        "        Returns:\n",
        "            Feature map de taille (16, 16, n_filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y24JRu6FwBbV"
      },
      "outputs": [],
      "source": [
        "class QuanvolutionalLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, n_filters: int = 8, n_qubits: int = 4, n_layers: int = 2):\n",
        "        super(QuanvolutionalLayer, self).__init__()\n",
        "\n",
        "        self.n_filters = n_filters\n",
        "\n",
        "        self.filters = nn.ModuleList([\n",
        "            QuantumFilter(n_qubits, n_layers) for _ in range(n_filters)\n",
        "        ])\n",
        "\n",
        "        total_quantum_params = sum(f.params.numel() for f in self.filters)\n",
        "        print(f\"  QuanvolutionalLayer: {n_filters} filters × {n_qubits} qubits × {n_layers} layers\")\n",
        "        print(f\"  Total quantum parameters: {total_quantum_params}\")\n",
        "\n",
        "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        H, W = image.shape[:2]\n",
        "        patch_size = 2\n",
        "        stride = 2\n",
        "\n",
        "        out_h = (H - patch_size) // stride + 1  \n",
        "        out_w = (W - patch_size) // stride + 1  \n",
        "\n",
        "        features_list = []\n",
        "\n",
        "        for filter_idx in range(self.n_filters):\n",
        "            print(f\"  Processing filter {filter_idx + 1}/{self.n_filters}...\", end='\\r')\n",
        "\n",
        "            filter_output = torch.zeros(out_h, out_w)\n",
        "\n",
        "            for i in range(out_h):\n",
        "                for j in range(out_w):\n",
        "                    # Extraction du patch 2x2\n",
        "                    h_start = i * stride\n",
        "                    w_start = j * stride\n",
        "                    patch = image[h_start:h_start+patch_size, w_start:w_start+patch_size, 0]\n",
        "\n",
        "                    patch_flat = patch.flatten()\n",
        "\n",
        "                    feature = self.filters[filter_idx].forward(patch_flat)\n",
        "                    filter_output[i, j] = feature\n",
        "\n",
        "            features_list.append(filter_output)\n",
        "\n",
        "        print()  \n",
        "\n",
        "        # Stack pour obtenir (n_filters, 16, 16) puis permute vers (16, 16, n_filters)\n",
        "        feature_map = torch.stack(features_list, dim=0)  # Shape: (n_filters, 16, 16)\n",
        "        feature_map = feature_map.permute(1, 2, 0)  # Shape: (16, 16, n_filters)\n",
        "\n",
        "        return feature_map.float()\n",
        "\n",
        "    def get_all_params(self) -> List[torch.Tensor]:\n",
        "        return [f.get_params() for f in self.filters]\n",
        "\n",
        "    def set_all_params(self, params_list: List[torch.Tensor]):\n",
        "        for i, params in enumerate(params_list):\n",
        "            self.filters[i].set_params(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    Quanvolutional Neural Network complète\n",
        "\n",
        "    PARAMÈTRES OPTIMISABLES:\n",
        "    1. Paramètres quantiques (VQC):\n",
        "       - n_filters × n_layers × 2 × n_qubits paramètres\n",
        "       - Exemple: 8 × 2 × 2 × 4 = 128 paramètres quantiques\n",
        "\n",
        "    2. Paramètres classiques (FC layers):\n",
        "       - FC1: (256×n_filters) × 128 + 128 bias\n",
        "       - FC2: 128 × 64 + 64 bias\n",
        "       - FC3: 64 × n_classes + n_classes bias\n",
        "       - Exemple pour n_filters=8, n_classes=10:\n",
        "         (2048×128 + 128) + (128×64 + 64) + (64×10 + 10) = 270,666 paramètres\n",
        "\n",
        "    TOTAL: ~128 quantiques + ~270k classiques = ~270k paramètres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Convertit RGB en grayscale\n",
        "\n",
        "        Args:\n",
        "            image: Tensor de taille (32, 32, 3)\n",
        "\n",
        "        Returns:\n",
        "            Tensor de taille (32, 32, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "        Forward pass complet\n",
        "\n",
        "        Args:\n",
        "            x: Batch d'images de taille (batch_size, 32, 32, 3)\n",
        "\n",
        "        Returns:\n",
        "            Probabilités de classe (batch_size, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duoimBC6ti5m"
      },
      "outputs": [],
      "source": [
        "class QuanNN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_filters: int = 8, n_classes: int = 10,\n",
        "                 n_qubits: int = 4, n_layers: int = 2):\n",
        "        super(QuanNN, self).__init__()\n",
        "\n",
        "        self.n_filters = n_filters\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Initializing QuanNN Model\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Couche quanvolutionnelle\n",
        "        self.quanv_layer = QuanvolutionalLayer(n_filters, n_qubits, n_layers)\n",
        "\n",
        "        flatten_dim = 256 * n_filters\n",
        "\n",
        "        # Couches fully connected classiques\n",
        "        self.fc1 = nn.Linear(flatten_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, n_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # Calcul du nombre de paramètres\n",
        "        quantum_params = sum(p.numel() for p in self.quanv_layer.parameters())\n",
        "        classical_params = sum(p.numel() for p in [*self.fc1.parameters(),\n",
        "                                                     *self.fc2.parameters(),\n",
        "                                                     *self.fc3.parameters()])\n",
        "        total_params = quantum_params + classical_params\n",
        "\n",
        "        print(f\"\\n  Parameter Breakdown:\")\n",
        "        print(f\"  {'─'*58}\")\n",
        "        print(f\"  Quantum (VQC) parameters:    {quantum_params:>10,}\")\n",
        "        print(f\"  Classical (FC) parameters:   {classical_params:>10,}\")\n",
        "        print(f\"  {'─'*58}\")\n",
        "        print(f\"  TOTAL trainable parameters:  {total_params:>10,}\")\n",
        "        print(f\"  {'='*60}\\n\")\n",
        "\n",
        "    def rgb_to_grayscale(self, image: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        if image.shape[2] == 3:\n",
        "            # Formule de conversion: 0.299*R + 0.587*G + 0.114*B\n",
        "            weights = torch.tensor([0.299, 0.587, 0.114], dtype=image.dtype)\n",
        "            gray = torch.sum(image * weights, dim=2, keepdim=True)\n",
        "            return gray\n",
        "        return image\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = x.shape[0]\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            print(f\"Processing image {i+1}/{batch_size}\")\n",
        "\n",
        "            # Step 1: Conversion en grayscale\n",
        "            img = x[i]\n",
        "            gray_img = self.rgb_to_grayscale(img)\n",
        "\n",
        "            # Step 2-5: Application de la couche quanvolutionnelle\n",
        "            feature_map = self.quanv_layer.forward(gray_img)\n",
        "            print(f\"  Shape after quanv_layer: {feature_map.shape}\")\n",
        "\n",
        "            # Step 6: Flatten - Reshape en vecteur 1D\n",
        "            # feature_map shape: (16, 16, n_filters)\n",
        "            # After flatten: (16 * 16 * n_filters) = (256 * n_filters)\n",
        "            features = feature_map.reshape(-1)  # Flatten all dimensions\n",
        "            print(f\"  Shape after flatten: {features.shape}\")\n",
        "\n",
        "            outputs.append(features)\n",
        "\n",
        "        # Stack all outputs\n",
        "        batch_features = torch.stack(outputs).float()\n",
        "        print(f\"Shape after stacking batch features: {batch_features.shape}\")\n",
        "\n",
        "        # Fully connected layers\n",
        "        h1 = self.relu(self.fc1(batch_features))\n",
        "        h2 = self.relu(self.fc2(h1))\n",
        "        logits = self.fc3(h2)\n",
        "\n",
        "        # Softmax for probabilities\n",
        "        probs = self.softmax(logits)\n",
        "\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rko7UtMPtfn9"
      },
      "outputs": [],
      "source": [
        "def test_quannn(model: QuanNN, test_loader: DataLoader, device='cpu'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    test_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n=== Testing QuanNN ===\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            print(f\"Testing batch {batch_idx + 1}/{len(test_loader)}...\", end='\\r')\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calcul de la loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Prédictions\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Statistiques\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Sauvegarde pour matrice de confusion\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calcul des métriques\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "\n",
        "    print(f\"\\n\\n{'='*50}\")\n",
        "    print(f\"Test Results:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    return accuracy, all_predictions, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu3WtEVWtcdB"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, class_names=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibKJB4qftWe0"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(train_losses, train_accs, val_losses=None, val_accs=None):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss\n",
        "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
        "    if val_losses:\n",
        "        ax1.plot(val_losses, label='Val Loss', marker='s')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Accuracy\n",
        "    ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
        "    if val_accs:\n",
        "        ax2.plot(val_accs, label='Val Accuracy', marker='s')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    print(\"Training history saved as 'training_history.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvG9P3I5tUyK"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model: QuanNN, test_loader: DataLoader,\n",
        "                         class_names=None, n_images=10):\n",
        "    model.eval()\n",
        "    images, labels = next(iter(test_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images[:n_images])\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "    # Affichage\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(min(n_images, len(images))):\n",
        "        img = images[i].cpu().numpy()\n",
        "\n",
        "        # Conversion pour affichage\n",
        "        if img.shape[2] == 3:\n",
        "            img = img.astype(np.uint8)\n",
        "        else:\n",
        "            img = img[:, :, 0].astype(np.uint8)\n",
        "\n",
        "        axes[i].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
        "\n",
        "        true_label = class_names[labels[i]] if class_names else labels[i].item()\n",
        "        pred_label = class_names[predictions[i]] if class_names else predictions[i].item()\n",
        "\n",
        "        color = 'green' if predictions[i] == labels[i] else 'red'\n",
        "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions_visualization.png')\n",
        "    print(\"Predictions visualization saved as 'predictions_visualization.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45fvQRY4tThM"
      },
      "outputs": [],
      "source": [
        "def load_cifar10_subset(n_classes=3, n_train=100, n_test=30):\n",
        "    print(f\"\\n=== Loading CIFAR-10 Subset ===\")\n",
        "    print(f\"Classes: {n_classes}, Train samples: {n_train}, Test samples: {n_test}\")\n",
        "\n",
        "    # Transformation: redimensionne à 32x32 (déjà la bonne taille pour CIFAR-10)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x * 255)  # Pixel values [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Chargement de CIFAR-10\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                           download=True, transform=transform)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                          download=True, transform=transform)\n",
        "\n",
        "    # Sélection des n_classes premières classes\n",
        "    class_indices_train = [i for i, label in enumerate(trainset.targets) if label < n_classes]\n",
        "    class_indices_test = [i for i, label in enumerate(testset.targets) if label < n_classes]\n",
        "\n",
        "    # Sous-échantillonnage\n",
        "    selected_train = class_indices_train[:n_train]\n",
        "    selected_test = class_indices_test[:n_test]\n",
        "\n",
        "    # Création des sous-ensembles\n",
        "    train_subset = torch.utils.data.Subset(trainset, selected_train)\n",
        "    test_subset = torch.utils.data.Subset(testset, selected_test)\n",
        "\n",
        "    # Noms des classes\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck'][:n_classes]\n",
        "\n",
        "    print(f\"Loaded: {len(train_subset)} train, {len(test_subset)} test samples\")\n",
        "    print(f\"Classes: {class_names}\\n\")\n",
        "\n",
        "    return train_subset, test_subset, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CtWbou9tSKX"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model: QuanNN, test_loader: DataLoader,\n",
        "                         class_names=None, n_images=10):\n",
        "    model.eval()\n",
        "    images, labels = next(iter(test_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images[:n_images])\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "    # Affichage\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(min(n_images, len(images))):\n",
        "        img = images[i].cpu().numpy()\n",
        "\n",
        "        # Conversion pour affichage\n",
        "        if img.shape[2] == 3:\n",
        "            img = img.astype(np.uint8)\n",
        "        else:\n",
        "            img = img[:, :, 0].astype(np.uint8)\n",
        "\n",
        "        axes[i].imshow(img, cmap='gray' if len(img.shape) == 2 else None)\n",
        "\n",
        "        true_label = class_names[labels[i]] if class_names else labels[i].item()\n",
        "        pred_label = class_names[predictions[i]] if class_names else predictions[i].item()\n",
        "\n",
        "        color = 'green' if predictions[i] == labels[i] else 'red'\n",
        "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', color=color)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions_visualization.png')\n",
        "    print(\"Predictions visualization saved as 'predictions_visualization.png'\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbeKmaJ8tQkZ"
      },
      "outputs": [],
      "source": [
        "def load_mnist_subset(n_classes=3, n_train=100, n_test=30):\n",
        "    print(f\"\\n=== Loading MNIST Subset ===\")\n",
        "    print(f\"Classes: {n_classes}, Train samples: {n_train}, Test samples: {n_test}\")\n",
        "\n",
        "    # Transformation: redimensionne à 32x32 et convertit en RGB avec le bon format (H, W, C)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),  # Donne (1, 32, 32)\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x),  # (3, 32, 32)\n",
        "        transforms.Lambda(lambda x: x.permute(1, 2, 0)),  # (32, 32, 3) - AJOUT CRUCIAL\n",
        "        transforms.Lambda(lambda x: x * 255)  # Pixel values [0, 255]\n",
        "    ])\n",
        "\n",
        "    # Chargement de MNIST\n",
        "    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                         download=True, transform=transform)\n",
        "    testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "    # Sélection des n_classes premières classes\n",
        "    class_indices_train = [i for i, label in enumerate(trainset.targets) if label < n_classes]\n",
        "    class_indices_test = [i for i, label in enumerate(testset.targets) if label < n_classes]\n",
        "\n",
        "    # Sous-échantillonnage\n",
        "    selected_train = class_indices_train[:n_train]\n",
        "    selected_test = class_indices_test[:n_test]\n",
        "\n",
        "    # Création des sous-ensembles\n",
        "    train_subset = torch.utils.data.Subset(trainset, selected_train)\n",
        "    test_subset = torch.utils.data.Subset(testset, selected_test)\n",
        "\n",
        "    class_names = [str(i) for i in range(n_classes)]\n",
        "\n",
        "    print(f\"Loaded: {len(train_subset)} train, {len(test_subset)} test samples\")\n",
        "    print(f\"Classes: {class_names}\\n\")\n",
        "\n",
        "    return train_subset, test_subset, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ZHeTuItO5T"
      },
      "outputs": [],
      "source": [
        "def save_model(model: QuanNN, filepath='quannn_model.pth'):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),  # Contient TOUS les paramètres\n",
        "        'quantum_params': model.quanv_layer.get_all_params(),\n",
        "        'n_filters': model.n_filters,\n",
        "        'n_classes': model.n_classes\n",
        "    }, filepath)\n",
        "    print(f\"✓ Model saved to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qB9QmrAtNJ9"
      },
      "outputs": [],
      "source": [
        "def load_model(filepath='quannn_model.pth', n_qubits=4, n_layers=2):\n",
        "    checkpoint = torch.load(filepath)\n",
        "\n",
        "    model = QuanNN(\n",
        "        n_filters=checkpoint['n_filters'],\n",
        "        n_classes=checkpoint['n_classes'],\n",
        "        n_qubits=n_qubits,\n",
        "        n_layers=n_layers\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.quanv_layer.set_all_params(checkpoint['quantum_params'])\n",
        "\n",
        "    print(f\"✓ Model loaded from {filepath}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1qbhuwftLq4"
      },
      "outputs": [],
      "source": [
        "def print_parameter_details(model: QuanNN):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DETAILED PARAMETER ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n📊 QUANTUM PARAMETERS (VQC):\")\n",
        "    print(\"─\"*70)\n",
        "    for idx, qfilter in enumerate(model.quanv_layer.filters):\n",
        "        params = qfilter.params\n",
        "        print(f\"  Filter {idx+1}:\")\n",
        "        print(f\"    Shape: {params.shape} = (layers={params.shape[0]}, \"\n",
        "              f\"rotations={params.shape[1]}, qubits={params.shape[2]})\")\n",
        "        print(f\"    Total: {params.numel()} parameters\")\n",
        "        print(f\"    Requires grad: {params.requires_grad}\")\n",
        "        print(f\"    Example values (first layer, RY rotations):\")\n",
        "        print(f\"      {params[0, 0, :].detach().numpy()}\")\n",
        "        if idx == 2:  # Limite l'affichage\n",
        "            print(f\"    ... ({model.n_filters - 3} more filters)\")\n",
        "            break\n",
        "\n",
        "    quantum_total = sum(f.params.numel() for f in model.quanv_layer.filters)\n",
        "    print(f\"\\n  ✓ Total Quantum Params: {quantum_total}\")\n",
        "\n",
        "    print(\"\\n📊 CLASSICAL PARAMETERS (Fully Connected):\")\n",
        "    print(\"─\"*70)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'fc' in name:\n",
        "            print(f\"  {name}:\")\n",
        "            print(f\"    Shape: {param.shape}\")\n",
        "            print(f\"    Total: {param.numel()} parameters\")\n",
        "            print(f\"    Requires grad: {param.requires_grad}\")\n",
        "\n",
        "    classical_total = sum(p.numel() for name, p in model.named_parameters() if 'fc' in name)\n",
        "    print(f\"\\n  ✓ Total Classical Params: {classical_total}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"GRAND TOTAL: {quantum_total + classical_total:,} trainable parameters\")\n",
        "    print(f\"  - Quantum: {quantum_total:,} ({100*quantum_total/(quantum_total+classical_total):.2f}%)\")\n",
        "    print(f\"  - Classical: {classical_total:,} ({100*classical_total/(quantum_total+classical_total):.2f}%)\")\n",
        "    print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJls2P4VtKbe"
      },
      "outputs": [],
      "source": [
        "def run_complete_experiment(dataset='cifar10', n_filters=4, n_classes=3,\n",
        "                           n_train=60, n_test=20, n_epochs=3, batch_size=2):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"QuanNN - Complete Experiment\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Chargement des données\n",
        "    if dataset == 'cifar10':\n",
        "        train_data, test_data, class_names = load_cifar10_subset(\n",
        "            n_classes=n_classes, n_train=n_train, n_test=n_test\n",
        "        )\n",
        "    elif dataset == 'mnist':\n",
        "        train_data, test_data, class_names = load_mnist_subset(\n",
        "            n_classes=n_classes, n_train=n_train, n_test=n_test\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Dataset must be 'cifar10' or 'mnist'\")\n",
        "\n",
        "    # Création des dataloaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # 2. Création du modèle\n",
        "    print(f\"\\nCreating QuanNN model...\")\n",
        "    print(f\"  - Quantum filters: {n_filters}\")\n",
        "    print(f\"  - Classes: {n_classes}\")\n",
        "    print(f\"  - VQC layers: 2\")\n",
        "\n",
        "    model = QuanNN(n_filters=n_filters, n_classes=n_classes,\n",
        "                   n_qubits=4, n_layers=2)\n",
        "\n",
        "    # Affichage de l'architecture\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  - Total trainable parameters: {total_params:,}\")\n",
        "\n",
        "    # 3. Entraînement\n",
        "    train_losses, train_accs, val_losses, val_accs = train_quannn(\n",
        "        model, train_loader, val_loader=test_loader,\n",
        "        n_epochs=n_epochs, lr=0.01\n",
        "    )\n",
        "\n",
        "    # 4. Test final\n",
        "    accuracy, predictions, labels = test_quannn(model, test_loader)\n",
        "\n",
        "    # 5. Visualisations\n",
        "    print(\"\\n=== Generating Visualizations ===\")\n",
        "    plot_confusion_matrix(labels, predictions, class_names)\n",
        "    visualize_predictions(model, test_loader, class_names, n_images=10)\n",
        "\n",
        "    # 6. Rapport de classification\n",
        "    print(\"\\n=== Classification Report ===\")\n",
        "    print(classification_report(labels, predictions,\n",
        "                                target_names=class_names,\n",
        "                                zero_division=0))\n",
        "\n",
        "    # 7. Sauvegarde du modèle\n",
        "    save_model(model, f'quannn_{dataset}_model.pth')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Experiment Complete!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nFiles generated:\")\n",
        "    print(f\"  - confusion_matrix.png\")\n",
        "    print(f\"  - training_history.png\")\n",
        "    print(f\"  - predictions_visualization.png\")\n",
        "    print(f\"  - quannn_{dataset}_model.pth\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return model, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FGXia_putIPV"
      },
      "outputs": [],
      "source": [
        "def train_quannn(model: QuanNN, train_loader: DataLoader,\n",
        "                 val_loader: DataLoader = None, n_epochs: int = 5,\n",
        "                 lr: float = 0.01, device='cuda'):\n",
        "    \"\"\"\n",
        "    Entraîne le modèle QuanNN avec suivi complet\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    print(\"\\n=== Training QuanNN ===\\n\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # === TRAINING ===\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{n_epochs}] - Training...\")\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calcul de la loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistiques\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f}\", end='\\r')\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_accuracy)\n",
        "\n",
        "        print(f\"\\n  Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "        # === VALIDATION ===\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            print(f\"  Validating...\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_accuracy = 100 * val_correct / val_total\n",
        "            val_loss_avg = val_loss / len(val_loader)\n",
        "            val_losses.append(val_loss_avg)\n",
        "            val_accs.append(val_accuracy)\n",
        "\n",
        "            print(f\"  Val Loss: {val_loss_avg:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        print(f\"{'-'*60}\\n\")\n",
        "\n",
        "    print(\"=== Training Complete ===\\n\")\n",
        "\n",
        "    # Affichage de l'historique\n",
        "    if val_loader:\n",
        "        plot_training_history(train_losses, train_accs, val_losses, val_accs)\n",
        "    else:\n",
        "        plot_training_history(train_losses, train_accs)\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywyQbvWtFPX",
        "outputId": "dd421937-d550-45ac-906c-ea60a1a752c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QuanNN - Testing Options\n",
            "============================================================\n",
            "\n",
            "Choose an option:\n",
            "  1. Quick test (CIFAR-10, 3 classes, 60 train/20 test)\n",
            "  2. MNIST test (3 classes, 60 train/20 test)\n",
            "  3. Extended CIFAR-10 (5 classes, 200 train/50 test)\n",
            "  4. Load and test existing model\n",
            "  5. Simple forward pass demo\n",
            "\n",
            "Enter choice (1-5): 2\n",
            "\n",
            "[Option 2] MNIST Test\n",
            "\n",
            "============================================================\n",
            "QuanNN - Complete Experiment\n",
            "============================================================\n",
            "\n",
            "=== Loading MNIST Subset ===\n",
            "Classes: 3, Train samples: 20, Test samples: 10\n",
            "Loaded: 20 train, 10 test samples\n",
            "Classes: ['0', '1', '2']\n",
            "\n",
            "\n",
            "Creating QuanNN model...\n",
            "  - Quantum filters: 2\n",
            "  - Classes: 3\n",
            "  - VQC layers: 2\n",
            "\n",
            "============================================================\n",
            "Initializing QuanNN Model\n",
            "============================================================\n",
            "    QuantumFilter initialized: 16 trainable quantum parameters\n",
            "    QuantumFilter initialized: 16 trainable quantum parameters\n",
            "  QuanvolutionalLayer: 2 filters × 4 qubits × 2 layers\n",
            "  Total quantum parameters: 32\n",
            "\n",
            "  Parameter Breakdown:\n",
            "  ──────────────────────────────────────────────────────────\n",
            "  Quantum (VQC) parameters:            32\n",
            "  Classical (FC) parameters:       74,115\n",
            "  ──────────────────────────────────────────────────────────\n",
            "  TOTAL trainable parameters:      74,147\n",
            "  ============================================================\n",
            "\n",
            "  - Total trainable parameters: 74,147\n",
            "\n",
            "=== Training QuanNN ===\n",
            "\n",
            "Epoch [1/2] - Training...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Batch 10/10 - Loss: 1.2970\n",
            "  Train Loss: 1.0962, Train Accuracy: 45.00%\n",
            "  Validating...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Val Loss: 1.0385, Val Accuracy: 40.00%\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch [2/2] - Training...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Batch 10/10 - Loss: 0.7122\n",
            "  Train Loss: 0.9723, Train Accuracy: 60.00%\n",
            "  Validating...\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "  Val Loss: 0.9741, Val Accuracy: 60.00%\n",
            "------------------------------------------------------------\n",
            "\n",
            "=== Training Complete ===\n",
            "\n",
            "Training history saved as 'training_history.png'\n",
            "\n",
            "=== Testing QuanNN ===\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "\n",
            "\n",
            "==================================================\n",
            "Test Results:\n",
            "==================================================\n",
            "Test Loss: 0.9741\n",
            "Test Accuracy: 60.00% (6/10)\n",
            "==================================================\n",
            "\n",
            "\n",
            "=== Generating Visualizations ===\n",
            "Confusion matrix saved as 'confusion_matrix.png'\n",
            "Processing image 1/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Processing image 2/2\n",
            "\n",
            "  Shape after quanv_layer: torch.Size([16, 16, 2])\n",
            "  Shape after flatten: torch.Size([512])\n",
            "Shape after stacking batch features: torch.Size([2, 512])\n",
            "Predictions visualization saved as 'predictions_visualization.png'\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.40      0.57         5\n",
            "           1       0.50      1.00      0.67         4\n",
            "           2       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.60        10\n",
            "   macro avg       0.50      0.47      0.41        10\n",
            "weighted avg       0.70      0.60      0.55        10\n",
            "\n",
            "✓ Model saved to quannn_mnist_model.pth\n",
            "\n",
            "============================================================\n",
            "Experiment Complete!\n",
            "============================================================\n",
            "\n",
            "Files generated:\n",
            "  - confusion_matrix.png\n",
            "  - training_history.png\n",
            "  - predictions_visualization.png\n",
            "  - quannn_mnist_model.pth\n",
            "\n",
            "\n",
            "\n",
            "============================================================\n",
            "Done!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"QuanNN - Testing Options\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nChoose an option:\")\n",
        "    print(\"  1. Quick test (CIFAR-10, 3 classes, 60 train/20 test)\")\n",
        "    print(\"  2. MNIST test (3 classes, 60 train/20 test)\")\n",
        "    print(\"  3. Extended CIFAR-10 (5 classes, 200 train/50 test)\")\n",
        "    print(\"  4. Load and test existing model\")\n",
        "    print(\"  5. Simple forward pass demo\")\n",
        "\n",
        "    choice = input(\"\\nEnter choice (1-5): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        # Test rapide avec CIFAR-10\n",
        "        print(\"\\n[Option 1] Quick CIFAR-10 Test\")\n",
        "        model, accuracy = run_complete_experiment(\n",
        "            dataset='cifar10',\n",
        "            n_filters=4,\n",
        "            n_classes=3,\n",
        "            n_train=60,\n",
        "            n_test=20,\n",
        "            n_epochs=3,\n",
        "            batch_size=2\n",
        "        )\n",
        "\n",
        "    elif choice == '2':\n",
        "        # Test avec MNIST\n",
        "        print(\"\\n[Option 2] MNIST Test\")\n",
        "        model, accuracy = run_complete_experiment(\n",
        "            dataset='mnist',\n",
        "            n_filters=2,\n",
        "            n_classes=3,\n",
        "            n_train=20,\n",
        "            n_test=10,\n",
        "            n_epochs=2,\n",
        "            batch_size=2\n",
        "        )\n",
        "\n",
        "    elif choice == '3':\n",
        "        # Test étendu\n",
        "        print(\"\\n[Option 3] Extended CIFAR-10 Test\")\n",
        "        print(\"WARNING: This will take significantly longer!\")\n",
        "        confirm = input(\"Continue? (y/n): \").strip().lower()\n",
        "        if confirm == 'y':\n",
        "            model, accuracy = run_complete_experiment(\n",
        "                dataset='cifar10',\n",
        "                n_filters=8,\n",
        "                n_classes=5,\n",
        "                n_train=200,\n",
        "                n_test=50,\n",
        "                n_epochs=5,\n",
        "                batch_size=2\n",
        "            )\n",
        "\n",
        "    elif choice == '4':\n",
        "        # Chargement et test d'un modèle existant\n",
        "        print(\"\\n[Option 4] Load Existing Model\")\n",
        "        filepath = input(\"Enter model path (default: quannn_cifar10_model.pth): \").strip()\n",
        "        if not filepath:\n",
        "            filepath = 'quannn_cifar10_model.pth'\n",
        "\n",
        "        try:\n",
        "            model = load_model(filepath)\n",
        "\n",
        "            # Chargement des données de test\n",
        "            _, test_data, class_names = load_cifar10_subset(n_classes=3, n_test=20)\n",
        "            test_loader = DataLoader(test_data, batch_size=2, shuffle=False)\n",
        "\n",
        "            # Test\n",
        "            accuracy, predictions, labels = test_quannn(model, test_loader)\n",
        "            plot_confusion_matrix(labels, predictions, class_names)\n",
        "            visualize_predictions(model, test_loader, class_names)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Model file '{filepath}' not found!\")\n",
        "\n",
        "    elif choice == '5':\n",
        "        # Démo simple du forward pass\n",
        "        print(\"\\n[Option 5] Simple Forward Pass Demo\")\n",
        "\n",
        "        # Création d'une image factice\n",
        "        print(\"\\nCreating random test image (32x32x3)...\")\n",
        "        test_img = torch.rand(1, 32, 32, 3) * 255\n",
        "\n",
        "        # Création du modèle\n",
        "        print(\"Creating QuanNN model (4 filters, 3 classes)...\")\n",
        "        model = QuanNN(n_filters=4, n_classes=3, n_qubits=4, n_layers=2)\n",
        "        # Affichage détaillé des paramètres\n",
        "        print_parameter_details(model)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            output = model(test_img)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Results:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Output shape: {output.shape}\")\n",
        "        print(f\"Class probabilities: {output[0].numpy()}\")\n",
        "        print(f\"Predicted class: {torch.argmax(output[0]).item()}\")\n",
        "        print(f\"Confidence: {torch.max(output[0]).item():.2%}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Forward pass avec gradient\n",
        "        output = model(test_img)\n",
        "        loss = -torch.log(output[0, 0])  # Simple loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        print(\"\\n✓ Gradient check:\")\n",
        "        has_quantum_grad = any(f.params.grad is not None for f in model.quanv_layer.filters)\n",
        "        has_classical_grad = model.fc1.weight.grad is not None\n",
        "\n",
        "        print(f\"  Quantum params have gradients: {has_quantum_grad}\")\n",
        "        print(f\"  Classical params have gradients: {has_classical_grad}\")\n",
        "\n",
        "        if has_quantum_grad:\n",
        "            print(f\"\\n  Example quantum gradient (Filter 1, Layer 1, RY):\")\n",
        "            print(f\"    {model.quanv_layer.filters[0].params.grad[0, 0, :].numpy()}\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"\\nInvalid choice! Running default demo...\")\n",
        "        print(\"\\n[Default] Simple Forward Pass Demo\")\n",
        "        test_img = torch.rand(1, 32, 32, 3) * 255\n",
        "        model = QuanNN(n_filters=4, n_classes=3, n_qubits=4, n_layers=2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(test_img)\n",
        "\n",
        "        print(f\"\\nOutput probabilities: {output[0].numpy()}\")\n",
        "        print(f\"Predicted class: {torch.argmax(output[0]).item()}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
